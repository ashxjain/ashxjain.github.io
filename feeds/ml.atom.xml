<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>ashxjain</title><link href="https://ashxjain.github.io/" rel="alternate"></link><link href="https://ashxjain.github.io/feeds/ml.atom.xml" rel="self"></link><id>https://ashxjain.github.io/</id><updated>2020-12-06T00:00:00+05:30</updated><entry><title>RCNN Family</title><link href="https://ashxjain.github.io/rcnn.html" rel="alternate"></link><published>2020-12-06T00:00:00+05:30</published><updated>2020-12-06T00:00:00+05:30</updated><author><name>Ashish Jain</name></author><id>tag:ashxjain.github.io,2020-12-06:rcnn.html</id><summary type="html">&lt;p&gt;Understanding RCNN models and its variants&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The story of RCNN actually starts from this paper: &lt;a href="https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf"&gt;Selective Search for Object Recognition. &lt;/a&gt;SSOR addresses the problem of generating possible object locations for use in object recognition. In turn, SSOR depends on &lt;a href="http://people.cs.uchicago.edu/~pff/papers/seg-ijcv.pdf"&gt;Efficient Graph-based Image Segmentation&lt;/a&gt; to find it's first proposals. This uses Computer Vision and not AI for segmentation, basically splitting the color channels&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;RCNN&lt;/strong&gt;: Manually segmented images into multiple region proposals (using CV algorithms) and then passed each proposal through AlexNET and then merge the segmented images. So for each image, AlexNET was run 2000 times! Also, The selective search algorithm is a fixed algorithm. Therefore, no learning
  is happening at that stage. This could lead to the generation of bad candidate region proposals.&lt;img src="images/RCNNInitial.png" alt="RCNNInitial" style="zoom: 33%;" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Fast RCNN&lt;/strong&gt;: Optimized RCNN by not passing Region proposals to AlexNet 2000 times, but rather just one time. Here we just have to figure out to map proposed region to convolved output
  &lt;img src="images/FastRCNN.png" alt="FastRCNN" style="zoom: 33%;" /&gt;
  &lt;img src="images/FastRCNNArch.png" alt="FastRCNNArch" style="zoom:33%;" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The network first processes the whole image with several convolutional (conv) and max pooling layers to produce a conv feature map.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Then, for each object proposal a region of interest (RoI) pooling layer extracts a fixed-length feature vector from the feature map (7x7). This is done because later we are converting 2D to 1D (FC layers) and 1D expects fixed size of input. Hence we convert the conv ROI input to fixed dim 7x7.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Each feature vector is fed into a sequence of fully connected (fc) layers that finally branch into two sibling output layers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;one that produces softmax probability estimates over K object classes plus a catch-all “background” class and&lt;/li&gt;
&lt;li&gt;another layer that outputs four real-valued numbers for each of the K object classes. Each set of 4 values encodes refined bounding-box positions for one of the K classes.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;RoI&lt;/strong&gt; Pooling layer uses maxpooling to convert the features inside any valid region of interest into a small feature map with resolution of 7x7. 
&lt;img src="images/FastRCNNArch2.png" alt="FastRCNNArch2" style="zoom: 33%;" /&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;So, above we see that input image is fed to conv net and then we get ROI from the proposal method (in this case it is SSOR), ROI pooling is done to make feature maps of resolution 7x7. Then FCs predicts two outputs. One is the class prediction and another one is the adjustments. Adjustments are nothing but adjustments to be done to original proposal.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ROI Pooling: Divide the ROI in such a way that we get final dimensions and then max pool in those divided blocks
    &lt;img src="images/ROI1.png" alt="ROI1" style="zoom: 25%;" /&gt; &lt;img src="images/ROI2.png" alt="ROI2" style="zoom:25%;" /&gt;
  &lt;img src="images/ROI3.png" alt="ROI3" style="zoom:25%;" /&gt; &lt;img src="images/ROI4.png" alt="ROI4" style="zoom:25%;" /&gt;
    &lt;img src="images/ROI5.png" alt="ROI5" style="zoom:25%;" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To increase resolution to 7x7 say from 4x3, then we just to interpolation i.e. 4x3 -&amp;gt; interpolated to -&amp;gt; 8x6, but we because we need 7x7, just repeat last column to get 8x7, then perform ROI pooling&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Above shows one example of a region proposal made by SSOR. But actual bounding boxes are not always well alligned, like shown below. Proposed bbox might be the dotted lines. This scaled to the closest edge and then we get well aligned bbox. Obviously this introduces more errors and that will be solved later
    &lt;img alt="ROIMisAlign" src="images/ROIMisAlign.png" /&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Also, above division of boxes can be done the way we like i.e. first box is 2x3, it could've been 3x3, etc. Fix the method and then train the model&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Faster RCNN&lt;/strong&gt;:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fast RCNN uses SSOR (internally uses EGIS), which has its problems, here it is fixed by using a neural network to predict these region proposals. This is RPN (Region Proposal Network)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;So the network will predict classes and since the network has learnt in its layers Edges/Gradients/Textures/Patterns, same can be used for RPN. Hence we branch out the network to predict RPs and then merge it back after few convolutions to get classes &amp;amp; boxes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Here RPN itself can predict the class as it knows the object. So RPN's output is the region proposal and the objectness. Infact it doesn't just say if the obj is present or not. It gives two output:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Foreground (object)&lt;/th&gt;
&lt;th&gt;Background (object's background)&lt;/th&gt;
&lt;th&gt;Note&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;Ignore&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Allow&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;Allow&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Punish!&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RPN:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First the picture goes through conv layers and feature maps are extracted&lt;/li&gt;
&lt;li&gt;Then a sliding window is used in RPN for each location over the feature map. Here sliding window means going through each block of final conv i.e. say in Yolo we had 13x13 divisions, so going through each block from 169 blocks (13x13) is called sliding window. In YoloV2, we only go through on network, but here it is different. Here we first get objectness (i.e. Fb/Bg), x,y,h,w (region proposal bbox, not final bbox). Then through another network we get classes &amp;amp; final bbox&lt;/li&gt;
&lt;li&gt;Unlike YoloV2 where our final feature map is 13x13, here it is based on input size. But we stop at the scale of 16 i.e. if input size is 1024, then our final feature map size will be 64x64.&lt;/li&gt;
&lt;li&gt;Here in the feature maps we'll have 9 anchor boxes per block. In YoloV2, we had 5 anchor boxes. Also anchor boxes are not found using K-means, instead they have fixed anchor boxes&lt;/li&gt;
&lt;li&gt;For each location, 9 anchor boxes are used (3 scales of 128, 256 and 512, and 3 aspect ratios of 1:1, 1:2 and 2:1) for generating region proposals. This means we have the following anchor boxes:
    | Scales | Aspect Ratios | Anchor box |
| ------ | ----- | ------------- |
| 128 | 1:1 | 128x128 |
|  | 2:1 | 256x128 |
| | 1:2 | 128x256 |
| 256 | 1:1 | 256x256 |
|  | 2:1 | 512x256 |
| | 1:2 | 256x512 |
| 512 | 1:1 | 512x512 |
|  | 2:1 | 1024x512 |
| | 1:2 | 512x1024 |&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;cls&lt;/strong&gt; layer outputs &lt;strong&gt;2x&lt;/strong&gt;9 scores whether is object or not for 9 anchors. &lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;reg&lt;/strong&gt; layer outputs 4x9 scores for coordinates for 9 anchor boxes.&lt;/li&gt;
&lt;li&gt;So if we compare YoloV2 with this network, we can understand this better:
    |  | YoloV2 | RPN |
| ------ | ----- | ------------- |
| Anchor Boxes | 5 | 9 |
| Technique to find anchor box | KMeans | Ratios |
| Prediction Output | Objectness (1value per anchor box) | Objectness (2[fg/bg] per anchor box) |
|| x,y,h,w (4 values per anchor box) | Dx,Dy,Dh,Dw (4 values per anchor box), this is region proposals (not final box). It is deltas and not actual value. So Dx,Dy is how much anchor box center. Dh,Dw is how much of anchor box to crop |
|| class predictions | no class predictions |&lt;/li&gt;
&lt;li&gt;With a size of WxH feature map, there are WH9 anchors in total. This means, we can have W != H.&lt;/li&gt;
&lt;li&gt;We then feed the region proposals to the RoI layer of the Fast R-CNN.
  &lt;img alt="RCNNRPN" src="images/RCNNRPN.png" /&gt;&lt;/li&gt;
&lt;li&gt;Faster RCNN (or others) have three main types of networks:&lt;/li&gt;
&lt;li&gt;Head (say ResNet50 etc)&lt;/li&gt;
&lt;li&gt;Region Proposal Network&lt;/li&gt;
&lt;li&gt;Classification Network&lt;/li&gt;
&lt;li&gt;Software flow:
  &lt;img src="images/FasterRCNNFlow.png" alt="FasterRCNNFlow" style="zoom: 67%;" /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Anchor Generation Layer:&lt;/strong&gt; This layer generates a fixed number of “anchors” (bounding boxes) by first generating 9 anchors of different scales and aspect ratios and then replicating these anchors by translating them across uniformly spaced grid points spanning the input image.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Proposal Layer:&lt;/strong&gt; Transform the anchors according to the bounding box regression coefficients to generate transformed anchors. Then prune the number of anchors by applying non-maximum suppression (see Appendix) using the probability of an anchor being a foreground region&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Anchor Target Layer&lt;/strong&gt;: The goal of the anchor target layer is to produce a set of “good” anchors and the corresponding foreground/background labels and target regression coefficients to train the Region Proposal Network. The output of this layer is only used to train the RPN network and is not used by the classification layer. Given a set of anchors (produced by the anchor generation layer, the anchor target layer identifies promising foreground and background anchors. Promising foreground anchors are those whose overlap with some ground truth box is higher than a threshold. Background boxes are those whose overlap with any ground truth box is lower than a threshold. The anchor target layer also outputs a set of bounding box regressors i.e., a measure of how far each anchor target is from the closest bounding box. These regressors only make sense for the foreground boxes as there is no notion of “closest bounding box” for a background box.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RPN Loss:&lt;/strong&gt; The RPN loss function is the metric that is minimized during optimization to train the RPN network. The loss function is a combination of:&lt;/li&gt;
&lt;li&gt;The proportion of bounding boxes produced by RPN that are correctly classified as foreground/background&lt;/li&gt;
&lt;li&gt;Some distance measure between the predicted and target regression coefficients.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Proposal Target Layer:&lt;/strong&gt; The goal of the proposal target layer is to prune the list of anchors produced by the proposal layer and produce &lt;em&gt;class specific&lt;/em&gt; bounding box regression targets that can be used to train the classification layer to produce good class labels and regression targets&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ROI Pooling Layer:&lt;/strong&gt; Implements a spatial transformation network that samples the input feature map given the bounding box coordinates of the region proposals produced by the proposal target layer. These coordinates will generally not lie on integer boundaries, thus interpolation based sampling is required.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Classification Layer:&lt;/strong&gt; The classification layer takes the output feature maps produced by the ROI Pooling Layer and passes them through a series of convolutional layers. The output is fed through two fully connected layers. The first layer produces the class probability distribution for each region proposal and the second layer produces a set of class specific bounding box regressors.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Classification Loss:&lt;/strong&gt; Similar to RPN loss, classification loss is the metric that is minimized during optimization to train the classification network. During back propagation, the error gradients flow to the RPN network as well, so training the classification layer modifies the weights of the RPN network as well. We’ll have more to say about this point later. The classification loss is a combination of:&lt;/li&gt;
&lt;li&gt;The proportion of bounding boxes produced by RPN that are correctly classified (as the correct object class)&lt;/li&gt;
&lt;li&gt;Some distance measure between the predicted and target regression coefficients.&lt;/li&gt;
&lt;li&gt;We saw above in our comparison table that we predict delta values. Why? We can that by understanding what &lt;strong&gt;Anchor Generation Layer&lt;/strong&gt; does&lt;/li&gt;
&lt;li&gt;The anchor generation layer produces a set of bounding boxes (called “anchor boxes”) of varying sizes and aspect ratios spread all over the input image. These bounding boxes are the same for all images i.e., they are agnostic of the content of an image. Some of these bounding boxes will enclose foreground objects while most won’t. The goal of the RPN network is to learn to identify which of these boxes are good boxes – i.e., likely to contain a foreground object and to produce target regression coefficients, which when applied to an anchor box turns the anchor box into a better bounding box (fits the enclosed foreground object more closely).&lt;/li&gt;
&lt;li&gt;Our objective is to find bounding boxes in the image. These have rectangular shape and can come in different sizes and aspect ratios. Imagine we were trying to solve the problem knowing beforehand that there are two objects on the image. The first idea that comes to mind is to train a network that returns 8 values: &lt;strong&gt;two xmin, ymin, xmax, ymax&lt;/strong&gt;
    &lt;strong&gt;​​tuples&lt;/strong&gt; defining a bounding box for each object.&lt;/li&gt;
&lt;li&gt;This approach has some fundamental problems. For example, images may have different sizes and aspect ratios, having a good model trained to predict raw coordinates can turn out to be very complicated (if not impossible). Another problem is invalid predictions: when predicting xmin and xmax we have to somehow enforce that xmin &amp;lt; xmax.&lt;/li&gt;
&lt;li&gt;It turns out that there is a simpler approach to predicting bounding boxes by learning to predict offsets from reference boxes. We take a reference box xcenter, ycenter, width, height and learn to predict Δ x_center, Δy_center, Δwidth, Δheight  , which are usually small values that tweak the reference box to better fit what we want.&lt;/li&gt;
&lt;li&gt;The diagram below demonstrates how these anchor boxes are generated.&lt;/li&gt;
&lt;li&gt;The region proposal layer has two goals:&lt;/li&gt;
&lt;li&gt;From a list of anchors, identify background and foreground anchors&lt;/li&gt;
&lt;li&gt;Modify the position, width and height of the anchors by applying a set of “regression coefficients” to &lt;strong&gt;improve the quality of the anchors&lt;/strong&gt; (for example, make them fit the boundaries of objects better)&lt;/li&gt;
&lt;li&gt;The region proposal layer consists of a Region Proposal &lt;em&gt;Network&lt;/em&gt; and three layers:&lt;ul&gt;
&lt;li&gt;Proposal Layer&lt;/li&gt;
&lt;li&gt;Anchor Target Layer and&lt;/li&gt;
&lt;li&gt;Proposal Target Layer.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RPN Diagram:
  &lt;img src="images/RPN.png" alt="RPN" style="zoom: 400%;" /&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Details about each layer &amp;amp; loss:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The region proposal layer runs feature maps produced by the head network through a convolutional layer (called rpn_net in code) followed by RELU. The output of rpn_net is run through two (1,1) kernel convolutional layers to produce background/foreground class scores and probabilities and corresponding bounding box regression coefficients. The stride length of the head network matches the stride used while generating the anchors, so the number of anchor boxes are in 1-1 correspondence with the information produced by the region proposal network&lt;/li&gt;
&lt;li&gt;(number of anchor boxes = number of class scores = number of bounding box regression coefficients = &lt;img alt="\frac{w}{16}\times\frac{h}{16}\times9" src="http://www.telesens.co/wp-content/ql-cache/quicklatex.com-cafeb41929226bc13d570966070b1313_l3.svg" /&gt;)&lt;/li&gt;
&lt;li&gt;The proposal layer takes the anchor boxes produced by the anchor generation layer and prunes the number of boxes by applying non-maximum suppression based on the foreground scores (see appendix for details). It also generates transformed bounding boxes by applying the regression coefficients generated by the RPN to the corresponding anchor boxes.&lt;/li&gt;
&lt;li&gt;The goal of the anchor target layer is to select promising anchors that can be used to train the RPN network to:&lt;/li&gt;
&lt;li&gt;distinguish between foreground and background regions and&lt;/li&gt;
&lt;li&gt;generate good bounding box regression coefficients for the foreground boxes.&lt;/li&gt;
&lt;li&gt;It is useful to first look at how the RPN Loss is calculated. This will reveal the information needed to calculate the RPN loss which makes it easy to follow the operation of the Anchor Target Layer. To do generate good bboxes from a set of anchor boxes, the RPN layer must learn to classify an anchor box as background or foreground and calculate the regression coefficients to modify the position, width and height of a foreground anchor box to make it a “better” foreground box (fit a foreground object more closely). RPN Loss is formulated in such a way to encourage the network to learn this behaviour.&lt;/li&gt;
&lt;li&gt;RPN loss is a sum of the classification loss and bounding box regression loss. The classification loss uses cross entropy loss to penalize incorrectly classified boxes and the regression loss uses a function of the distance between the true regression coefficients (calculated using the closest matching ground truth box for a foreground anchor box) and the regression coefficients predicted by the network (see rpn_bbx_pred_net in the RPN network architecture diagram).&lt;/li&gt;
&lt;li&gt;The feature vector is then passed through two fully connected layers – bbox_pred_net and cls_score_net. The cls_score_net layer produces the class scores for each bounding box (which can be converted into probabilities by applying softmax). The bbox_pred_net layer produces the class specific bounding box regression coefficients which are combined with the original bounding box coordinates produced by the proposal target layer to produce the final bounding boxes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In Summary:
    &lt;img src="images/FastRCNNSummary.png" alt="FastRCNNSummary" style="zoom:150%;" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Faster R-CNN uses a CNN feature extractor to extract image features. Then it uses a CNN region proposal network to create region of interests (RoIs). We apply RoI pooling to warp them into fixed dimension. It is then feed into fully connected layers to make classification and boundary box prediction.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Mask R-CNN&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Faster R-CNN builds all the ground works for feature extractions and ROI proposals. At first sight, performing image segmentation may require more detail analysis to colorize the image segments. By surprise, not only we can piggyback on this model, the extra work required is pretty simple. After the ROI pooling, we add 2 more convolution layers to build the mask.
    &lt;img src="images/MaskRCNN.png" alt="MaskRCNN" style="zoom:150%;" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Example conv flow: Say input is 7x7x2048 and we need mask as 7x7x3 (say we have 3 classes), then we can just have two convs of 3x3x512 &amp;amp; 3x3x3. So mask output is divided into fixed blocks (like YoloV2), and mask of each class is a separate dim. So if there are 15 classes, then the output will be 7x7x15&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Infact we can even add another branch like how we did for mask to predict pose.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ROI Align:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Another major contribution of Mask R-CNN is the refinement of the ROI pooling. In ROI, the warping is digitalized (top left diagram below): the cell boundaries of the target feature map are forced to realign with the boundary of the input feature maps.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Therefore, each target cells may not be in the same size (bottom left diagram). Mask R-CNN uses &lt;strong&gt;ROI Align&lt;/strong&gt; which does not digitalize the boundary of the cells (top right) and make every target cell to have the same size (bottom right).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It also applies interpolation to calculate the feature map values within the cell better. For example, by applying interpolation, the maximum feature value on the top left is changed from 0.8 to 0.88 now.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="ROIAlign" src="images/ROIAlign.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A detailed explanation on ROI pooling can be found &lt;a href="https://towardsdatascience.com/understanding-region-of-interest-part-2-roi-align-and-roi-warp-f795196fc193"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ROI Align makes significant improvements in the accuracy&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary><category term="ml"></category><category term="rcnn"></category></entry><entry><title>SuperConvergence</title><link href="https://ashxjain.github.io/superconvergence.html" rel="alternate"></link><published>2020-12-06T00:00:00+05:30</published><updated>2020-12-06T00:00:00+05:30</updated><author><name>Ashish Jain</name></author><id>tag:ashxjain.github.io,2020-12-06:superconvergence.html</id><summary type="html">&lt;p&gt;Achieving SuperConvergence&lt;/p&gt;
&lt;h5&gt;General Notes&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;LR points:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Our loss function is somewhat like the following:&lt;/p&gt;
&lt;p&gt;&lt;img alt="LR-WD-BS" src="images/LR-WD-BS.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LR is multiplied by the gradient weight update &amp;amp; the weight decay averages over batch size. But say we move from Colab to AWS GPU (where we have higher GPUs), then our batch size will increase (say 8 times). Then we should make sure that LR is also increased respectively such that above equality is maintained.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Image Augmentation:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The main reason we do image aug is to mimic real world dataset, increasing accuracy, regularization etc are all outcome of image aug&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can use Blender. Look at blendswap, chocofur etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Plateus: L1/L2 regularization doesn't help with plateaus. Gradient perturbation, dropout, patch guassian does help&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To look at SOTA model of different dataset look at "paperswithcode benchmarks"&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;a href="https://arxiv.org/pdf/1708.07120.pdf"&gt;Very Fast Training of Neural Networks using Large Learning Rates&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[Abstract]: In this paper, we describe a phenomenon, which we called "super-convergence", here &lt;strong&gt;neural&lt;/strong&gt; &lt;strong&gt;networks can be trained an order of magnitude faster&lt;/strong&gt; than with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. 
  &lt;img alt="LeslieSmithPaperSnippet" src="images/LeslieSmithPaperSnippet.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As per paper, large learning rates gives regularization effects during training!&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Regularization" src="images/Regularization.png" /&gt;
  &lt;img alt="MutlipleRegularization" src="images/MutlipleRegularization.png" /&gt;&lt;/p&gt;
&lt;h4&gt;Evolution of OCP (One-Cycle-Policy) from Simple LR&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In previous session (LR Finder), we found the best LR by doing the following:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Try range of LRs for a number of iterations&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Plot LR vs Loss graph&lt;/li&gt;
&lt;li&gt;The best LR is where we see &lt;strong&gt;steepest reduction in loss&lt;/strong&gt;. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Best-LR" src="images/Best-LR.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Selecting a good starting learning rate is merely the first step. In order to efficiently train a robust model, we will need to gradually decrease the learning rate during training. If the learning rate remains unchanged during the course of training, it might be too large to converge and cause the loss function to fluctuate around the local minimum. The approach is to use a higher learning rate to quickly reach the regions of (local) minima during the initial training stage, and set a smaller learning rate as training progresses in order to explore &lt;strong&gt;“deeper and more thoroughly”&lt;/strong&gt; in the region to find the minimum.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Piecewise Constant&lt;/strong&gt;:
  &lt;img alt="PiecewiseConstant" src="images/PiecewiseConstant.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Benefits: As we have discussed through our variable value example, say a variable needs to be at 0.3456789. &lt;/p&gt;
&lt;p&gt;But if our updates are governed by &lt;img alt="LaTeX: -\gamma\times\binom{\partial L}{\partial w_i}" src="https://canvas.instructure.com/equation_images/-%255Cgamma%255Ctimes%255Cbinom%257B%255Cpartial%2520L%257D%257B%255Cpartial%2520w_i%257D" /&gt;, and it can only go as low as some multiple of 0.001, &lt;/p&gt;
&lt;p&gt;then we can only achieve &lt;strong&gt;0.345&lt;/strong&gt;. For us to achieve the remaining 0.000&lt;strong&gt;6789&lt;/strong&gt;, we need to reduce&lt;/p&gt;
&lt;p&gt;the magnitude order of updates by 1 decimal unit every time. In Piecewise constant we reduce the learning rate gradually, keep it constant for a few epochs. Normally, you'd see magnitude drop by 10 every 25-50 epoch.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Problem: We don't know when to change the LR. Have to use Hyper-parameter Grid Search to find that. One will need extraordinary resources to do this, and this isn't practical, except outside academia or large MNCs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reduce LR on Plateau&lt;/strong&gt;: Simpler solution to above problem. But then they are not yet easy to pick by the way. For example, you really need to understand the difference between &lt;strong&gt;rel&lt;/strong&gt; vs &lt;strong&gt;abs&lt;/strong&gt; threshold_mode. So in a way, we'll have to do grid search here to experiment with different parameters &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Warm-Up Strategies&lt;/strong&gt;: Our variables are initialized randomly, shouldn't we give the network some time to "warm-up" and "align" the variables in the right direction before we actually train them? Different approaches:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Constant warmup&lt;/strong&gt;: In constant warm up, you train the model with a small learning rate for few epochs (say 5 epochs -&amp;gt; again grid search to get this number!) and then increase the learning rate to “k times learning rate”. However, this approach causes a spike in the training error when the learning rate is changed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gradual warmup&lt;/strong&gt;: As the name suggests, you start with a small learning rate and then gradually increase it by a constant for each epoch till it reaches “k times learning rate”. This approach helps the model to perform better with huge batch sizes (8k in this example) , which is in par with the training error of the model trained with smaller batches.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CLR (Cyclic LR)&lt;/strong&gt;: The essence of the learning rate policy comes from the observation that &lt;strong&gt;increasing the LR&lt;/strong&gt; might have a &lt;strong&gt;short term negative effect&lt;/strong&gt; and yet achieve a &lt;strong&gt;longer-term beneficial effect&lt;/strong&gt;. This observation leads to the idea of letting the LR vary within a range of values rather than adopting a stepwise fixed or exponentially decreasing value.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Short-term negative effect: During initialization, some points might be closer to their optimal value, but if we increase the LR by a large value, then they might displace to a value far from the optimal value&lt;/li&gt;
&lt;li&gt;Longer-term beneficial effect: But displacing from an optimal value could be that we just moved it from one local minima to another better minima. i.e. we want the values to be in a minima, where slight changes (change in dataset), doesn't make network perform bad:
  &lt;img alt="BetterMinima" src="images/BetterMinima.png" /&gt;&lt;/li&gt;
&lt;li&gt;Increasing the learning rate will force the model to jump to a different part of the weight space if the current area is “spikey”. It is better to avoid spikey area, because although we might good accuracy there, it won't be Robust! Ideal minima is the second image (with big mouth)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;One Cycle Policy (OCP)&lt;/strong&gt;: In the paper “&lt;a href="https://arxiv.org/abs/1803.09820"&gt;A disciplined approach to neural network hyper-parameters: Part 1 — learning rate, batch size, momentum, and weight decay &lt;/a&gt;” , Leslie Smith describes the approach to set hyper-parameters (namely learning rate, momentum and weight decay) and batch size. In particular, he suggests 1 Cycle policy to apply learning rates.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Before looking at OCP, one must under LR range test to find min &amp;amp; max LRs. Basically, plot LR against accuracy (i.e. train with increasing LR) and find a range from the plot where you think accuracy is pretty consistent:
  &lt;img alt="LRRangeTest" src="images/LRRangeTest.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now coming back to OCP: The author recommends doing one cycle of learning rate of 2 steps of &lt;strong&gt;equal length&lt;/strong&gt; (actually not equal, as we reduce LR significantly in last few epochs). We choose the maximum learning rate using a range test. We use a lower learning rate as 1/5th or 1/10th of the maximum learning rate (Rohan usually uses 1/8, also not getting min from graph like above). We go from a lower learning rate to a higher learning rate in step 1 and back to a lower learning rate in step 2. We pick this cycle length slightly lesser than the total number of epochs to be trained. And in the last remaining iterations, we annihilate the learning rate way below the lower learning rate value(1/10 th or 1/100 th).
  &lt;img alt="OCP" src="images/OCP.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The motivation behind this is that, during the middle of learning when the learning rate is higher, the learning rate works as a regularisation method and keep the network from overfitting. This helps the network to avoid steep areas of loss and land better flatter minima.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As in the figure, We start at a learning rate 0.08 and make a step of 41 epochs to reach a learning rate of 0.8, then make another step of 41 epochs where we go back to a learning rate 0.08. Then we make another 13 epochs to reach 1/10th of lower learning rate bound(0.08).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How do we know at which iteration we should have the peak? Also how long to train the network?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This will come up as intuition. But we'll see that we can train our network very very fast! Maybe in just, say, 4 epochs. 2k to 10k is a good iteration to work with. We can just start with 2k. But what about the peak, should it be at 1k? -&amp;gt; No, originally it was proposed to be in middle, but now it has changed:&lt;/p&gt;
&lt;p&gt;&lt;img alt="PeaksForOCP" src="images/PeaksForOCP.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Momentum and learning rate are closely related. It can be seen in the weight update equation for SGD that the momentum has a similar impact as the learning rate on weight updates. The author found in their experiments that reducing the momentum when the learning rate is increasing gives better results. This supports the intuition that in that part of the training, we want the SGD to quickly go in new directions to find a better minima, so the new gradients need to be given more weight. [Rohan doesn't set this momentum, as he didn't see much difference, we can just use a fixed momentum]
  &lt;img alt="OCPMomentum" src="images/OCPMomentum.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Does OCP provide us higher accuracy in practice? &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Maybe, but in practice NO. We know that if gradients are constant and weight decay is constant, then our equation is this: [LR/BatchSize]. So if we increase BS, then we can increase LR. Hence with higher BS, we can go with higher LR. So if we increase BS by 10, then we can increase LR also by 10. When we find LRmax from LR range test, better to take a point which has higher LR and almost near to highest accuracy (not necessarily highest accuracy)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Usual process:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Find LRmax using range test&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;LRmin (Rohan says it is better to have it as LRmax/8)&lt;/li&gt;
&lt;li&gt;Iterations (start with 2K, once we find our peak, then you can train with 10K or higher)&lt;/li&gt;
&lt;li&gt;Peak can be found by trying with 600,800,1K,1.2K&lt;/li&gt;
&lt;li&gt;Annihilation, can be done say in between 1.6K - 2K&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Better to have it written in an excel sheet:
    &lt;img alt="OCPLRSearch" src="images/OCPLRSearch.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For different LR max values, have LRmin (diff values /5,/8,/10) and for different iterations with different annihilation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Then why use OCP?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It reduces the time it takes to reach "near" to your accuracy. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;It allows us to know if we are going right early on. &lt;/li&gt;
&lt;li&gt;It let us know what kind of accuracies we can target with a given model.&lt;/li&gt;
&lt;li&gt;It reduces the cost of training. &lt;/li&gt;
&lt;li&gt;It reduces the time to deploy!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Things one must achieve in AI:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Must know how to train object detection model from scratch&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Must know Superconvergence&lt;/li&gt;
&lt;li&gt;Must know how to train object recognition model from scratch. Will be useful for Transfer learning&lt;/li&gt;
&lt;li&gt;Take ResNET18/ResNET34  and take ImageNET, go to AWS train the model from scratch to the end, as close as possible to the SOTA. Thats it.&lt;/li&gt;
&lt;/ul&gt;</summary><category term="ml"></category></entry><entry><title>YOLO</title><link href="https://ashxjain.github.io/yolo.html" rel="alternate"></link><published>2020-12-06T00:00:00+05:30</published><updated>2020-12-06T00:00:00+05:30</updated><author><name>Ashish Jain</name></author><id>tag:ashxjain.github.io,2020-12-06:yolo.html</id><summary type="html">&lt;p&gt;Understanding Yolo model and its variants&lt;/p&gt;
&lt;h4&gt;Yolo V2&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Paper: https://arxiv.org/abs/1612.08242v1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Classification (Recognition): To identify cat vs dog; classify the object&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Detection: Bounding box along with classification; where the object is&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compared to V1: BatchNorm, Skip connection, Fully convolutional, High res classifier/detector etc&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Uses anchor boxes: Anchor box are set of fixed size boxes which are scaled to fit object in it. After scaling the anchor box for the object, the final box is called bounding box&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Strategy used to train faster:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Start with low res, for example if you are high res images are of 448x448 dims, then you start training with 56x56 i.e. scale down the image and then train. This will be 64 times faster!! (448/56 * 448/56)&lt;/li&gt;
&lt;li&gt;With initial low res training, we train initial layers for E/G/T/P&lt;/li&gt;
&lt;li&gt;Because the network is fully convolutionally, we can input images of different dimensions&lt;/li&gt;
&lt;li&gt;We already saw that we divide the image dims by its own dim, essentially making all the values between 0-1. This way Yolo will work on &lt;strong&gt;proportions&lt;/strong&gt; rather than actual sizes. This way find clusters of anchor boxes of same proportions using k-means clustering.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Output is 13x13x5x25:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Image is divided into 13x13 blocks. Each block's top-left corner coordinate is (0,0) &amp;amp; bottom-right coordinate is (1,1)&lt;/li&gt;
&lt;li&gt;5 = 5 anchor boxes&lt;/li&gt;
&lt;li&gt;25 = 5 + 20:&lt;/li&gt;
&lt;li&gt;5 = 4 + 1 = x,y,h,w + o&lt;ul&gt;
&lt;li&gt;x,y = centroid of bounding box (will be between 0-1), sigmoid is used here&lt;/li&gt;
&lt;li&gt;h,w = scaling factor of bounding box. Say anchor box dim is h1,w2, and ground truth can be close to achor box dim or very big. Hence h,w is used to get ground truth dim by using exponential func: &lt;code&gt;h1.e^h&lt;/code&gt;, &lt;code&gt;w1.e^w&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;o = objectness (is object present or not) ranges between 0-1. We choose only those which has o above a certain threshold&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;20 = 20 classes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="YoloV2-4D" src="images/YoloV2-4D.jpg" /&gt;
  * Anchor dimensions are picked using k-means clustering on the dimensions of original bounding boxes. Final anchor boxes are: (0.57273, 0.677385), (1.87446, 2.06253), (3.33843, 5.47434), (7.88282, 3.52778), (9.77052, 9.16828). What are these values?
&lt;em&gt; This is of the ratio 13:13 i.e. divide the image into 13x13 block and above is the dimension on that 13x13 coordinate system
&lt;/em&gt; Above is for 224x244 resolution. If the dimension is different, then network might not end with 13x13. For example, in YOT, final output is 26x26. Hence these values will change.
  * If the cell is offset from the top left corner of the image by cx, cy and the bounding box ground-truth/prior has width and height gw, gh then the predictions correspond to:
- bx = σ(tx) + cx, where σ is sigmoid
- by = σ(ty) + cy
- bw = gw.e^tw
- bh = gh.e^th
  * 13x13 will work for large resolution. But what about small resolution? For this they add a skip connection (passthrough) from 26x26 resolution layer to semi last layer:
&lt;img alt="yolov2_arch" src="https://www.researchgate.net/publication/336177198/figure/fig4/AS:809235726229506@1569948240717/The-architecture-of-YOLOv2.ppm" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Above, we can see that in pass through, in dim is 26x26x256 and out is 13x13x2048? How did that happen?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;26x26x256 ---reshape---&amp;gt; 13x13x1024 ----conv---&amp;gt; 1x1x1024x2048 -&amp;gt; 13x13x2048&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;And below passthrough, calculation is given, where output is 13x13x1024&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Both are stacked and we get 13x13x3072&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The pass-through layer concatenates the higher resolution features with the low-resolution features by stacking adjacent features into different channels&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Every 10 batches, the network chooses a random new image dimension size (multiples of 32) from 320x320 to 608x608. The anchor box dimensions will also need to scale up/down respectively.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The final model, called Darknet-19 has 19 convolution layers and 5 max-pooling layers. 1x1 convolutions are used to compress the feature representations between 3x3.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The network is first trained on classification for 160 epochs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After classification training, the last convolution layer is removed, and three 3x3 convolution layers with 1024 filters each followed by the final 1x1 convolution layer are added. The network is again trained for 160 epochs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;During training both, detection and classification datasets are mixed. When the network sees an image with detection label, full back-propagation is performed, else only the classification part is back-propagated.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DarkNet-19:
  &lt;img src="https://paperswithcode.com/media/methods/Screen_Shot_2020-06-24_at_12.38.12_PM.png" alt="drawing" width="400"/&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This table is designed to start at 224. If it starts at 416, it would end at 13x13.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Loss Function:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="YoloV2Loss" src="images/YoloV2Loss.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We need to compute losses for each Anchor Box (5 in total): ∑B represents this part.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We need to do this for each of the 13x13 cells where S = 13: ∑S2 represents this part.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;pij =&amp;gt; Classes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cij is also objectness, but that is used here to train the network to predict Cij&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;1ijobj is 1 when there is an object in the cell ii, else 0.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;1ijnoobj is 1 when there is no object in the cell ii, else 0. We need to do this to make sure we reduce confidence when there is no object as well.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;1iobj is 1 when there is a particular class is predicted, else 0.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;λs are constants. λ is highest for coordinates in order to focus more on detection (remember, we have already trained the network for recognition!)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We can also notice that wi, hi  are under square-root. This is done to penalise the smaller bounding boxes as we need to adjust them more.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Check out this table:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;var1&lt;/th&gt;
&lt;th&gt;var2&lt;/th&gt;
&lt;th&gt;(var1-var2)^2&lt;/th&gt;
&lt;th&gt;(sqrtvar1-sqrtvar2)^2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0.0300&lt;/td&gt;
&lt;td&gt;0.020&lt;/td&gt;
&lt;td&gt;9.99e-05&lt;/td&gt;
&lt;td&gt;0.001&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.0330&lt;/td&gt;
&lt;td&gt;0.022&lt;/td&gt;
&lt;td&gt;0.00012&lt;/td&gt;
&lt;td&gt;0.0011&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.0693&lt;/td&gt;
&lt;td&gt;0.046&lt;/td&gt;
&lt;td&gt;0.000533&lt;/td&gt;
&lt;td&gt;0.00233&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.2148&lt;/td&gt;
&lt;td&gt;0.143&lt;/td&gt;
&lt;td&gt;0.00512&lt;/td&gt;
&lt;td&gt;0.00723&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.8808&lt;/td&gt;
&lt;td&gt;0.587&lt;/td&gt;
&lt;td&gt;0.0862&lt;/td&gt;
&lt;td&gt;0.0296&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4.4920&lt;/td&gt;
&lt;td&gt;2.994&lt;/td&gt;
&lt;td&gt;2.2421&lt;/td&gt;
&lt;td&gt;0.1512&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For first 160 epochs, Lambda(coord) is set to 0, and then once it is trained for classification, we train it for detection&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;YOLO V3&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In YOLO-V2, they added passthrough and did stacking to support smaller resolution. Here, stacking is taken to next level
  &lt;img alt="YoloV2V3SSD" src="images/YoloV2V3SSD.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Feature extractor here is ResNET&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;V3 uses only Convolutional layers, not even the pooling layer! How can we avoid any pooling layer?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A 3x3 with a stride of 2.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If we start at 416 and end at 13, we have taken a total stride of 32 (416/13).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;COCO Dataset has 80 classes. So final output shall be: 13x13x&lt;strong&gt;3&lt;/strong&gt;x(4+1+&lt;strong&gt;80&lt;/strong&gt;) = 13x13x&lt;strong&gt;255&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;YoloV3 has &lt;strong&gt;3&lt;/strong&gt; Anchor Boxes! or &lt;strong&gt;9?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3 Anchor boxes for the resolutions: 52x52, 26x26, 13x13 -&amp;gt; So in total 9 Anchor boxes i.e. 3 anchor boxes in different scales!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="YoloV3Arch" src="images/YoloV3Arch.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;As seen above, no pooling layers, dim reduction is done using convolution layer with stride=2&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Starting from Scale-3: Post convolution, output size is 13x13x255&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;At Scale-2, here's the flow:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;13x13x1024 ----[conv 1x1x256]---&amp;gt;13x13x256----[upsample]----&amp;gt;26x26x256------&amp;gt;[concat with 26x26x512]----&amp;gt;26x26x768---[multi-convs]---&amp;gt;26x26x512---[conv 1x1x255]-----&amp;gt;26x26x255&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;At Scale-3, here's the flow:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;52x52x256----[concat 52x52x128]---&amp;gt;52x52x384---[multi-convs]----&amp;gt;52x52x256---[conv 1x1x255]---&amp;gt;52x52x255&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Upsampling is done as shown below:
  &lt;img alt="upsample" src="images/upsample.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="upsample-code" src="images/upsample-code.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Class Confidence:&lt;/li&gt;
&lt;li&gt;YoloV3 has an interesting take on Class probabilities. Normally you'd take a SoftMax of the output vector. This is based on the assumption that classes are mutually exclusive. &lt;em&gt;If it is a &lt;/em&gt;*Dog&lt;strong&gt;&lt;em&gt;, it cannot be a &lt;/em&gt;*Cat&lt;/strong&gt;!&lt;/li&gt;
&lt;li&gt;YOLOv3 asks a question, what if we have classes which are not mutually exclusive. If it is a &lt;strong&gt;&lt;em&gt;Person&lt;/em&gt;&lt;/strong&gt;, it may be a &lt;strong&gt;&lt;em&gt;Man&lt;/em&gt;&lt;/strong&gt; as well! So instead of SoftMax, v3 uses a sigmoid function.&lt;/li&gt;
&lt;li&gt;v3 makes predictions at 52x52, 26x26 and 13x13; alternatively, at &lt;strong&gt;strides of 8 (416/52), 16 (416/26) and 32 (416/13).&lt;/strong&gt;
    &lt;img alt="YoloV3PredictionScales" src="images/YoloV3PredictionScales.png" /&gt;&lt;/li&gt;
&lt;li&gt;v3 in total now predicts (52x52 + 26x26 + 13x13)*3 = &lt;strong&gt;10647&lt;/strong&gt; bounding boxes &lt;/li&gt;
&lt;li&gt;Output processing:&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Object Confidence Threshold&lt;/strong&gt;: We filter the boxes based on their objectness score. Say we only consider those boxes which have a value greater than some threshold&lt;/li&gt;
&lt;li&gt;Get all the proposed anchor boxes for a class&lt;/li&gt;
&lt;li&gt;Calculate (NMS) &lt;strong&gt;Non-maximum Suppression&lt;/strong&gt;: A technique that helps selects the best bunding box among overlapping proposals
  &lt;img alt="NMS" src="images/NMS.png" /&gt;&lt;/li&gt;
&lt;li&gt;Say there are 4 anchors boxes for a class: C1, C2, C3, C4&lt;/li&gt;
&lt;li&gt;Calculate the IOU amongst those boxes i.e C1's against C2,C3,C4. Similarly, C2's against C1,C3,C4 etc. Only consider boxes which has overlap (IOU) value above some threshold (say 40%) i.e. if anchor box is less than threshold, then the box will be of 2 different objects of same class (for example, two cars in same image)&lt;/li&gt;
&lt;li&gt;Group the achor boxes with IOU more than a threshold and only keep the anchor box with maximum confidence (i.e. found max features) and suppress (ignore) others&lt;/li&gt;
&lt;li&gt;NMS fails if there is too much overlap between two objects of same class. For example:
  &lt;img alt="NMSOverlap" src="images/NMSOverlap.png" /&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;YOLO V5&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;YOLOv4 has improved again in terms of accuracy (average precision) and speed (FPS), the two metrics we generally use to qualify an object detection algorithm.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There are &lt;strong&gt;4 apparent blocks, after the input image:&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Backbone&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Neck&lt;/li&gt;
&lt;li&gt;Dense Prediction—used in one-stage-detection algorithms such as YOLO, SSD, etc&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sparse Prediction—used in two-stage-detection algorithms such as Faster-R-CNN, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Backbone: &lt;strong&gt;Cross-Stage-Partial connections.&lt;/strong&gt; The idea here is to separate the current layer into 2 parts, one that will go through a block of convolutions, and one that won’t. Then, we aggregate the results. Here’s an example with DenseNet:
  &lt;img alt="YoloV4Backbone" src="images/YoloV4Backbone.png" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Neck: The purpose of the neck block is to &lt;strong&gt;add extra layers between the backbone and the head (dense prediction block)&lt;/strong&gt;. You might see that different feature maps from the different layers used.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="YoloV4Neck" src="images/YoloV4Neck.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;YoloV4 used a modified version of the &lt;a href="https://arxiv.org/pdf/1803.01534.pdf"&gt;PANet (Path Aggregation Network)&lt;/a&gt;. The idea is again to aggregate information to get higher accuracy. Rather than addition, it does concatenation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Another technique used is &lt;a href="https://arxiv.org/pdf/1807.06521.pdf"&gt;Spatial Attention Module (SAM) (Links to an external site.)&lt;/a&gt;. Attention mechanisms have been widely used in deep learning, and especially in recurrent neural networks. It is like SENet (Squeeze-Excitation Network)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="YoloV4SAM" src="images/YoloV4SAM.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="SENet" src="images/SENet.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Finally, &lt;a href="https://arxiv.org/pdf/1406.4729.pdf"&gt;Spatial Pyramid Pooling (SPP)&lt;/a&gt;, used in R-CNN networks and numerous other algorithms, is also used here.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="YoloV4SpatialPool" src="images/YoloV4SpatialPool.png" /&gt;&lt;/p&gt;</summary><category term="ml"></category><category term="yolo"></category></entry><entry><title>T3D [Twin Delayed DDPG] Reinforcement Learning</title><link href="https://ashxjain.github.io/t3d-reinforcement-learning.html" rel="alternate"></link><published>2020-04-04T00:00:00+05:30</published><updated>2020-04-04T00:00:00+05:30</updated><author><name>Ashish Jain</name></author><id>tag:ashxjain.github.io,2020-04-04:t3d-reinforcement-learning.html</id><summary type="html">&lt;p&gt;Understanding Implementation Of Twin Delayed DDPG (T3D)&lt;/p&gt;
&lt;p&gt;T3D is a reinforcement learning model, based on &lt;a href="https://arxiv.org/pdf/1602.01783.pdf"&gt;Asynchronous Advantage Actor-Critic Algorithm&lt;/a&gt; (A3C). But before we understand and implement T3D, let's get a quick understanding of what is reinforcement learning, what is A3C model and why to use A3C based models.&lt;/p&gt;
&lt;p&gt;In reinforcement learning, an agent/program is continuously learning from its environment. It learns on what to do, how to map situations to actions with the aim to maximize rewards it acheive by performing right actions for particular situations.&lt;/p&gt;
&lt;p&gt;As we all know about the Q equation derived from famous Bellman Equation, which is the basis for reinforcement learning:&lt;/p&gt;
&lt;p&gt;&lt;img alt="BellmanEqn" src="images/BellmanEqn.svg" /&gt;&lt;/p&gt;
&lt;p&gt;So in above equation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Q (s, a) = Q-value of being in state (s) and reaching state (s') by taking an action (a)&lt;/li&gt;
&lt;li&gt;R (s, a) = Reward you get after taking that action and reaching state (s') from state (s)&lt;/li&gt;
&lt;li&gt;γ (gamma) = the discounting factor (a hyperparameter), to balance the immediate reward and future reward&lt;/li&gt;
&lt;li&gt;Q&lt;sub&gt;max&lt;/sub&gt; (s', a') = max Q value across all actions (a') taken from state (s')&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Q-value can be considered as a value associated with a specific action. Max of multiple Q-values for multiple actions is what is considered as action for the agent.&lt;/p&gt;
&lt;p&gt;For solving complex problems, we use a Deep Q Network (DQN), to predict Q-values as opposed to using a value table based model.&lt;/p&gt;
&lt;p&gt;A DQN takes in state as input and outputs Q values for all possible actions.&lt;/p&gt;
&lt;p&gt;&lt;img alt="DQN" src="images/DQN.png" /&gt;&lt;/p&gt;
&lt;p&gt;Since there are discrete number of actions, it will not work for continuous action spaces. For example, it works fine if say a car's action is to move 5 degrees left or right or no movement at all. But if it has be a range like -5 to +5 degrees, then this will not work. Hence comes in A3C models.&lt;/p&gt;
&lt;p&gt;&lt;img alt="A3C" src="images/A3C.png" /&gt;&lt;/p&gt;
&lt;p&gt;A3C models is an extension to DQN model, where we have two models: Actor &amp;amp; Critic.&lt;/p&gt;
&lt;p&gt;Actor is trying to predict an action based on the current state (policy network), and critic is trying to predict the V-Values (max Q-Values) given the state and actions. Critic model ensures that the actor model takes right action as part of training process. To make it work for continuous action spaces, the value of actor model (max output) is actually used for training. This value defines the action value. More details on why actor-critic model and its training aspects is covered as part of T3D explanation.&lt;/p&gt;
&lt;p&gt;In T3D, twin stands for "2 Critic models", hence here we have 1 Actor, 2 Critic models.&lt;/p&gt;
&lt;p&gt;&lt;img alt="T3D-HighLevel" src="images/T3D-HighLevel.png" /&gt;&lt;/p&gt;
&lt;p&gt;Two critic models gives stability to our network. More explanation on this and how it is trained is covered step-by-step with actual implementation.&lt;/p&gt;
&lt;h2&gt;Step 1: Initialization&lt;/h2&gt;
&lt;p&gt;Import all the required libraries. A note on few important libraries:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pytorch.org"&gt;https://pytorch.org&lt;/a&gt;: We use PyTorch for our neural network implementation&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gym.openai.com"&gt;Gym&lt;/a&gt;: This provides a variety of environments like Atari, MuJoCo, etc for our reinforcement learning experiments&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/benelot/pybullet-gym"&gt;https://github.com/benelot/pybullet-gym&lt;/a&gt;: Library providing physics based environment for our experiment&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pybullet_envs&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;gym&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.nn&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;nn&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.nn.functional&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;F&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;gym&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;wrappers&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torch.autograd&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Variable&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;deque&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Step 2: Define Replay Memory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This is a fixed size array storing multiple experiences.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An experience (aka transition) is defined by the following:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;s: current state in which the agent is&lt;/li&gt;
&lt;li&gt;a: action the agent takes to go to next state&lt;/li&gt;
&lt;li&gt;s': new state agent reaches after taking an action (a)&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;r: reward an agent receive for going from state (s) to state (s') by taking action (a)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Initially, agent plays with the environment randomly and fills in replay memory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Then during training, a batch of experiences is sampled randomly to train the agent.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Also this memory is simultaneously filled as and when agent explores the environment.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If memory is full, then first entry is removed and new entry is added.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="ReplayMemory" src="images/ReplayMemory.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Replay memory size is usually initialised to a large number, in our case 1 Million, so that agent can learn from variety of experiences&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;ReplayBuffer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1e6&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storage&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;max_size&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ptr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;transition&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storage&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storage&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ptr&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transition&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ptr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ptr&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_size&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storage&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;transition&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;ind&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storage&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;batch_states&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_next_states&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_actions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_rewards&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; \
                &lt;span class="n"&gt;batch_dones&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;ind&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storage&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;batch_states&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;copy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="n"&gt;batch_next_states&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;copy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="n"&gt;batch_actions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;copy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="n"&gt;batch_rewards&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;copy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="n"&gt;batch_dones&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;copy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_states&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_next_states&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; \
                &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_actions&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_rewards&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; \
                &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_dones&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Above we define a &lt;code&gt;sample&lt;/code&gt; function, as during training this becomes our dataset. Here we randomly sample a &lt;strong&gt;batch&lt;/strong&gt; of experiences and use that as model inputs and for loss calculations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Step 3: Define Actor-Critic Models&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Following defines our network model for Actor &amp;amp; Critic. It is a simple dense network, with RELU used as activation layer.&lt;/li&gt;
&lt;li&gt;For Actor model, our input is state and output is actions. Hence we specify &lt;code&gt;state_dims&lt;/code&gt; and &lt;code&gt;action_dim&lt;/code&gt; in below code.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;code&gt;max_action&lt;/code&gt; is used to clamp the action value in case we add too much gaussian noise. More on this is explained further. So to limit the output in &lt;code&gt;-max_action&lt;/code&gt; to &lt;code&gt;+max_action&lt;/code&gt; range, we use &lt;code&gt;tanh&lt;/code&gt; to confine the network to &lt;code&gt;-1&lt;/code&gt; to &lt;code&gt;+1&lt;/code&gt; range and then multiply it with &lt;code&gt;max_action&lt;/code&gt;, thereby getting our output in the required range.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Actor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state_dims&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action_dim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_action&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# max_action is to clip in case we added too much noise&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Actor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# activate the inheritance&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;state_dims&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;400&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;400&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;300&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer_3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;300&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action_dim&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;max_action&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer_2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_action&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;For Critic model, since we need two models, we are definining them in same class but with different output variables. This way it is easy for us to write and maintain the code.&lt;/li&gt;
&lt;li&gt;Here, our input is state and action, hence we pass both &lt;code&gt;state_dims&lt;/code&gt; &amp;amp; &lt;code&gt;action_dim&lt;/code&gt; as part of initialisation. During training, input to this model is concatenation of both state and action.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Note:&lt;/strong&gt; we also define a separate network Q1, which is actually same as first critic network. This is used for loss calculation and updating weights of Actor model. More on this is covered in following steps.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Critic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state_dims&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action_dim&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Critic&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# activate the inheritance&lt;/span&gt;
        &lt;span class="c1"&gt;# First Critic Network&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;state_dims&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;action_dim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;400&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;400&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;300&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer_3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;300&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action_dim&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# Second Critic Network&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer_4&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;state_dims&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;action_dim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;400&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer_5&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;400&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;300&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer_6&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;300&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action_dim&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;# x - state, u - action&lt;/span&gt;
        &lt;span class="n"&gt;xu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 1 for vrtcl concatenation, 0 for Hzntl&lt;/span&gt;
        &lt;span class="c1"&gt;# forward propagation on first critic&lt;/span&gt;
        &lt;span class="n"&gt;x1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xu&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;x1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer_2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;x1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# forward propagation on second critic&lt;/span&gt;
        &lt;span class="n"&gt;x2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer_4&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xu&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;x2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer_5&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;x2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer_6&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x2&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;Q1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;# x - state, u - action&lt;/span&gt;
        &lt;span class="c1"&gt;# This is used for updating the Q values&lt;/span&gt;
        &lt;span class="n"&gt;xu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 1 for vrtcl concatenation, 0 for Hzntl&lt;/span&gt;
        &lt;span class="n"&gt;x1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xu&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;x1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer_2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;x1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Device selection:&lt;/strong&gt; If our model is trained on CPU, then below code should take care of setting &lt;code&gt;device='cpu'&lt;/code&gt;, similarly for GPU. That way we can write our code without specifically mentioning a particular device.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;cuda&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_available&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;cpu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;## Training our model&lt;/p&gt;
&lt;h2&gt;Step 4: Training Initializations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Our whole training process is built in a class. In this class, as part of &lt;code&gt;__init__&lt;/code&gt;, we initialize the following networks:
  &lt;img alt="T3D" src="images/T3D.png" /&gt;&lt;/li&gt;
&lt;li&gt;As part of initialization, Actor Target model weights are same as Actor model. Similary Critic Target models weight are same as correspoding Critic models.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;T3D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state_dims&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action_dim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_action&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# making sure our T3D class can work with any env&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Actor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;state_dims&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action_dim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actor_target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Actor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;state_dims&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action_dim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# initializing with model weights to keep the same&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actor_target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_state_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;state_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actor_optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;max_action&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;critic&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Critic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;state_dims&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action_dim&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;critic_target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Critic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;state_dims&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action_dim&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# initializing with model weights to keep the same&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;critic_target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_state_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;critic&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;state_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;critic_optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;critic&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Step 5: Action Selection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In every training iteration, as and when we sample batch of experiences from replay memory, our agent needs to take an action during that iteration. This is part of online training. The action which agent takes is selected by calling &lt;code&gt;select_action&lt;/code&gt;. Agent's current state is passed to Actor model to get next action. This way agent is getting trained as well simultaneously performing action.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;select_action&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# need to convert to numpy, for clipping&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cpu&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Step 6: Train Method&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Train method is defined with following arguments:&lt;/li&gt;
&lt;li&gt;replay_buffer: This is the replay memory in which we are storing the experiences&lt;/li&gt;
&lt;li&gt;iterations: Number of iterations to train the network&lt;/li&gt;
&lt;li&gt;batch_size: Number of experiences to be sampled from replay memory&lt;/li&gt;
&lt;li&gt;discount: Discounting factor used for calculating target Q-value which will be used for loss calculations&lt;/li&gt;
&lt;li&gt;tau: Hyperparameter used to update weights of target network from model network using Polyak Averaging&lt;/li&gt;
&lt;li&gt;policy_noise: Noise added to Actor Target output, when passed to Critic Target networks. This way we achieve exploration&lt;/li&gt;
&lt;li&gt;noise_clip: Clips the policy_noise to maintain it in a specific range&lt;/li&gt;
&lt;li&gt;policy_freq: Because the target network weights and Actor model weight updation is delayed, we define this parameter to control when to update the weights. If 2, then after every two iterations.&lt;/li&gt;
&lt;li&gt;First step in training is to randomly sample batch of experiences from replay memory.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Note:&lt;/strong&gt; the environment also provides &lt;code&gt;done&lt;/code&gt; variable to indicate if an episode is done or not.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;replay_buffer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;iterations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;discount&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.99&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;tau&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.005&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;policy_noise&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;noise_clip&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;policy_freq&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iterations&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# Sample from a batch of transitions (s, s&amp;#39;, a, r) from the memory&lt;/span&gt;
        &lt;span class="n"&gt;batch_states&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_next_states&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_actions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_rewards&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_dones&lt;/span&gt; \
            &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;replay_buffer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_states&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;next_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_next_states&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_actions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_rewards&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;done&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_dones&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Step 7: Perform Action In The Environment&lt;/h2&gt;
&lt;p&gt;Actor network predicts next action for the agent to take from current state. This is the step agent performs in the environment and is visible on the game/environment screen. And the resulting state and reward is all stored as a new experience in the replay memory. This step is just to proceed the agent in the game/environment and to add entry in the replay memory.&lt;/p&gt;
&lt;h2&gt;Step 8: Train Actor Network&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The main aim is to train Actor network as it provides next action to be performed in the environment.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;But to train actor network, we first need to get output from Critic network and hence we must first train Critic network. And Critic network is trained by Critic Target network, which in turn needs output from Actor Target network. So let's break this down and first see how to train Critic Network&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="T3D-Train-Order" src="images/T3D-Train-Order.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Step 7: Training Critic Network&lt;/h2&gt;
&lt;p&gt;Critic network takes in (s, a) from the batch. And outputs Q-value.&lt;/p&gt;
&lt;p&gt;For loss calculation we first need to find target Q-value. And that is calculated using Bellman's equation:&lt;/p&gt;
&lt;p&gt;&lt;img alt="BellmanEqn" src="images/BellmanEqn.svg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="T3D-CriticNetworkWeightsBackProp" src="images/T3D-CriticNetworkWeightsBackProp.png" /&gt;&lt;/p&gt;
&lt;h5&gt;Step 7.1: Calculating target Q-Value&lt;/h5&gt;
&lt;p&gt;So, we need the following to calculate target Q-value:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;R (s, a): reward for taking action (a) from current state (s), we have this value from our batch entry (experience)&lt;/li&gt;
&lt;li&gt;γ (gamma): Discounting factor is already passed as input, defined as hyperparameter&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Q&lt;sub&gt;max&lt;/sub&gt; (s', a'): This is the Q-value for next state (a') by performing next action (a'). To find this Q-value of a future state, we need target network. Because we know neural networks are good approximaters, we can use the same model for next state Q calculations&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We already have &lt;code&gt;R(s,a), γ , s'&lt;/code&gt; , but we need next action (a') to be performed from state (s')&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Step 7.1.1: Next Action (a')&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;We get next action from our Actor Target network:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;        &lt;span class="c1"&gt;# From the next state s&amp;#39;, the actor target plays the next action a&amp;#39;&lt;/span&gt;
        &lt;span class="n"&gt;next_action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actor_target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h5&gt;Step 7.1.2: Add Gaussian Noise To Next Action (a')&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;We add Gaussian noise to next action (a') and clamp it between &lt;code&gt;-max_action&lt;/code&gt; to &lt;code&gt;+max_action&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;This allows our agent to explore the environment and learn better.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;        &lt;span class="c1"&gt;# We add Gaussian noise to this next action a&amp;#39; and&lt;/span&gt;
        &lt;span class="c1"&gt;# we clamp it in a range of values supported by the environment&lt;/span&gt;
        &lt;span class="n"&gt;noise&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;next_action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal_&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;policy_noise&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;noise&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;noise&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clamp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;noise_clip&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;noise_clip&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;next_action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;next_action&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;noise&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clamp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_action&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h5&gt;Step 7.1.3: Fetch Q-Values From Both Critic Target Networks&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;So we pass (s', a') to Critic Target network and get the required Q value for target Q value calculations.&lt;/li&gt;
&lt;li&gt;But using a single Critic Target network's output, makes model too optimistic. And hence we use another Critic Target network, and take the minimum of both of the networks. So that we are not too optimistic with the Q-value, and it gives network enough time to learn Q-values and hence adds stability.&lt;/li&gt;
&lt;li&gt;Hence our target Q-value formula will now be: &lt;code&gt;Qt = r + gamma * min(Qt1, Qt2).&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;        &lt;span class="c1"&gt;# The two Critic targets take each the couple (s&amp;#39;, a&amp;#39;)&lt;/span&gt;
        &lt;span class="c1"&gt;# as input and return two Q values, Qt1(s&amp;#39;, a&amp;#39;) and&lt;/span&gt;
        &lt;span class="c1"&gt;# Qt2(s&amp;#39;, a&amp;#39;) as outputs&lt;/span&gt;
        &lt;span class="n"&gt;target_Q1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target_Q2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;critic_target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;next_action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# Keep the minimum of these two Q-Values&lt;/span&gt;
        &lt;span class="n"&gt;target_Q&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;target_Q1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target_Q2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Now that we have this Q-value from Critic Target Network, we calculate our final target Q-Value.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Note:&lt;/strong&gt; That we are only supposed to run this if the episode is over, which means we need to integrate &lt;code&gt;done&lt;/code&gt; here. Also, we must detach target Q-Value as it would create it's own computation graph without detaching Qt1/Qt2 from their own graph and hence complicating things.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;        &lt;span class="n"&gt;target_Q&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;discount&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;target_Q&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;detach&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h5&gt;Step 7.2: Predicting Q-Values from Critic Network&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Now that we have target Q-Value, let's get predicted Q-values from both the Critic networks and calculate critic loss.&lt;/li&gt;
&lt;li&gt;Critic Loss is combined mean squared loss (MSE) of Q-value from critic network 1 and target-Q &amp;amp; MSE of Q-value from critic network 2 and target-Q.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;        &lt;span class="c1"&gt;# Two critic models take (s, a) as input and return two Q-Vales&lt;/span&gt;
        &lt;span class="n"&gt;current_Q1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;current_Q2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;critic&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# Compute the critic loss&lt;/span&gt;
        &lt;span class="n"&gt;critic_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mse_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_Q1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target_Q&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mse_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_Q2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target_Q&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;We now backpropagte and update Critic Network weights.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;        &lt;span class="c1"&gt;# Backpropagate this critic loss and update the parameters&lt;/span&gt;
        &lt;span class="c1"&gt;# of two Critic models with an Adam optimizer&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;critic_optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zero_grad&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# initializing the gradients to zero&lt;/span&gt;
        &lt;span class="n"&gt;critic_loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# computing the gradients&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;critic_optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# performing weight updates&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Step 8: Actor Network Backpropagation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Now that we have Critic network updated with new weights. Once in every policy_freq (=2) iteration, we update Actor network weights.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="T3D-ActorNetworkGradAsc" src="images/T3D-ActorNetworkGradAsc.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Actor network uses Critic network 1 (Q1)'s output for loss calculation. This loss is maximized using Gradient Ascent. We maximize loss here because we want to maximize Q-value and max Q-value is the action taken by the agent.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="c1"&gt;# Once every two iterations, we update our Actor model by performing&lt;/span&gt;
    &lt;span class="c1"&gt;# gradient ASCENT on the output of the first Critic model&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;policy_freq&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# This is DPG part&lt;/span&gt;
        &lt;span class="n"&gt;actor_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;critic&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Q1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actor_optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grad_zero&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;actor_loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actor_optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Step 9: Target Networks Weights Updation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Once the actor network weights are updated, after next two iterations, target networks' weights are updated from their corresponding model networks using Polyak Averaging.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="T3D-PolyakAvg" src="images/T3D-PolyakAvg.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Polyak Averaging:&lt;/strong&gt; The essence of this equation is to take little of new weights and keep most of old weights. Tau is a very small number.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="PolyakAvgEqn" src="images/PolyakAvgEqn.svg" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Above equation can be rewritten as:
                                                                 W&lt;sub&gt;new&lt;/sub&gt; = (tau) W&lt;sub&gt;in&lt;/sub&gt; + (1 - tau) W&lt;sub&gt;old &lt;/sub&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; above W is actual weights and not delta of weights.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Here we are biased on old weights and expecting new weights to come in continously and take the network in right direction.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;            &lt;span class="c1"&gt;# Once in every two iterations, we update our Actor Target&lt;/span&gt;
            &lt;span class="c1"&gt;# by Polyak Averaging&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target_param&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;actor_target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;()):&lt;/span&gt;
                &lt;span class="n"&gt;target_param&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy_&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tau&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;tau&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;target_param&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# Once in every two iterations, we update our Critic Target&lt;/span&gt;
            &lt;span class="c1"&gt;# by Polyak Averaging&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target_param&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;critic&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;critic_target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;()):&lt;/span&gt;
                &lt;span class="n"&gt;target_param&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy_&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tau&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;tau&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;target_param&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is one iteration. We'll perform multiple iterations until we finish an episode or reach the end of iterations count.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Here's a summary in terms of first 4 iterations:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Iteration-1:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Select Action:&lt;/li&gt;
&lt;li&gt;Agent is started with initial state &lt;code&gt;s&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Agent selects new action using &lt;strong&gt;Actor Network&lt;/strong&gt; : &lt;code&gt;s -&amp;gt; [Actor] -&amp;gt; a&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;Agent reaches new state &lt;code&gt;s'&lt;/code&gt; after performing action &lt;code&gt;a&lt;/code&gt;. Also agent receives reward &lt;code&gt;r&lt;/code&gt; for reaching state &lt;code&gt;s'&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Store &lt;code&gt;[s, a, s', r]&lt;/code&gt; as experience in replay memory&lt;/li&gt;
&lt;li&gt;Randomly sample batch of experiences from replay memory. We'll consider single experience from batch data for understanding: &lt;code&gt;[s, a, s', r]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Train both the &lt;strong&gt;Critic Networks&lt;/strong&gt;:&lt;/li&gt;
&lt;li&gt;Predict Q-values:&lt;ul&gt;
&lt;li&gt;&lt;code&gt;(s, a) -&amp;gt; [Critic-1] -&amp;gt; Q-v1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;(s, a) -&amp;gt; [Critic-2] -&amp;gt; Q-v2&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Calculate Target Q values:&lt;ul&gt;
&lt;li&gt;Get next-action &lt;code&gt;a'&lt;/code&gt; from &lt;strong&gt;Target Actor Network&lt;/strong&gt;: &lt;code&gt;s' -&amp;gt; [Actor-Target] -&amp;gt; a'&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;(s', a') -&amp;gt; [Critic-1] -&amp;gt; Qt'-v1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;(s', a') -&amp;gt; [Critic-2] -&amp;gt; Qt'-v2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Get target Q-value: &lt;code&gt;Qt = r + (1-done)*gamma * min(Qt'-v1, Qt'-v2)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Calculate critic loss function, minimize it:&lt;ul&gt;
&lt;li&gt;&lt;code&gt;critic_loss = F.mse_loss(Q-v1, Qt) + F.mse_loss(Q-v2, Qt)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Perform backpropagation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Iteration-2:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Follow steps 1-3 as mentioned above.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Train &lt;strong&gt;Actor Network&lt;/strong&gt;:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Calculate actor loss: &lt;ul&gt;
&lt;li&gt;Get next-action &lt;code&gt;a'&lt;/code&gt; from &lt;strong&gt;Actor Network&lt;/strong&gt;: &lt;code&gt;s -&amp;gt; [Actor] -&amp;gt; a&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Get Q1 value from &lt;strong&gt;Critic Network 1&lt;/strong&gt;: &lt;code&gt;(s, a) -&amp;gt; [Critic-1] -&amp;gt; Q-v1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Actor loss: &lt;code&gt;actor_loss = -(Q-v1).mean()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Perform backpropagation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Iteration-3:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Follow steps 1-3 as mentioned above.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Iteration-4:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Follow steps 1-4 as mentioned above.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update Target Networks' weight by Polyak Averaging:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Actor Target Network&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Update weights from Actor Network&lt;/li&gt;
&lt;li&gt;Actor-Target&lt;sub&gt;new&lt;/sub&gt; = (tau) Actor&lt;sub&gt;new&lt;/sub&gt; + (1 - tau) Actor-Target&lt;sub&gt;old &lt;/sub&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Critic Target Network 1&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Update weights from Critic Network 1&lt;/li&gt;
&lt;li&gt;Critic-Target-1&lt;sub&gt;new&lt;/sub&gt; = (tau) Critic-1&lt;sub&gt;new&lt;/sub&gt; + (1 - tau) Critic-Target-1&lt;sub&gt;old &lt;/sub&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Critic Target Network 2&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Update weights from Critic Network 2&lt;/li&gt;
&lt;li&gt;Critic-Target-2&lt;sub&gt;new&lt;/sub&gt; = (tau) Critic-2&lt;sub&gt;new&lt;/sub&gt; + (1 - tau) Critic-Target-2&lt;sub&gt;old &lt;/sub&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary><category term="ml"></category><category term="reinforcement-learning"></category></entry></feed>